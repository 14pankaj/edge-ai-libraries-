image:
  registry: "intel/"
  backendTag: "core_1.2"
  pullPolicy: IfNotPresent

global:
  http_proxy:
  https_proxy:
  no_proxy:
  huggingface: 
    apiToken:
  EMBEDDING_MODEL_NAME:
  RERANKER_MODEL:
  LLM_MODEL:
  model_cache_path: "/tmp/model_cache"
  UI_NODEPORT: 
  pvc:
    size: 40Gi
  keeppvc: false # true  to persist models across multiple deployments
chatqna:
  name: chatqna-core
  service:
    type: ClusterIP
    port: 8888
  readinessProbe:
    httpGet:
      path: /v1/chatqna/health
      port: 8888
    initialDelaySeconds: 30
    periodSeconds: 30
  startupProbe:
    httpGet:
      path: /v1/chatqna/health
      port: 8888
    initialDelaySeconds: 2500 
    periodSeconds: 30
  env: 
    MAX_TOKENS: "1024"
    ENABLE_RERANK: "true"
    CACHE_DIR: "/tmp/model_cache"
    HF_DATASETS_CACHE:  "/tmp/model_cache"
    TMP_FILE_PATH: "/tmp/chatqna/documents"
    
gpu:
  enabled: false
  devices: /dev/dri
  group_add: $(stat -c "%g" /dev/dri/render*)
  device: "GPU"  #If the system has an integrated GPU, its id is always 0 (GPU.0). The GPU is an alias for GPU.0. If a system has multiple GPUs (for example, an integrated and a discrete Intel GPU) It is done by specifying GPU.1,GPU.0 as a __GPU_DEVICE__
  node:
    key:  #update as per the cluster node label key for GPU
    value:   #update as per the cluster node label value for GPU

uiService:
  name: chatqna-core-ui
  type: ClusterIP
  port: 80
